```{r setup, include=FALSE,message=FALSE,warning=FALSE}
# OPTIONS -----------------------------------------------
knitr::opts_chunk$set(echo = TRUE, 
                      warning=FALSE, 
                      message = FALSE)

# PACKAGES-----------------------------------------------
# Tutorial packages
library(vembedr)
library(skimr)
library(yarrr)
library(RColorBrewer)
library(GGally) 
library(tidyverse)
library(plotly)
library(readxl)
library(rvest)
library(biscale)
library(tidycensus)
library(cowplot)
library(units)
library(olsrr)
library(ggstatsplot)
library(DALEX)

data("iris")

```


# Basic model comparisons

You are now being asked to assess two (or more) models and decide "which is best". Here are some metrics we could use to compare models.

### Is the model appropriate?  LINE & influential outliers {.unnumbered}

There is no point comparing a model if one (or both) of them is not valid! So before carrying on, it's important to assess whether each of your models is valid in its own right. See the LINE tutorial for more.

1.  [**Linearity**]{.underline} **- COULD A CURVE FIT BETTER THAN A LINE?**
    -   *Assessed by visual inspection of the scatterplot and residual plot.* <br><br>
    -   If broken, apply a transformation to the RESPONSE and see if your new model does better <br><br>
2.  [**Independence**]{.underline} **- To be best of your knowledge, Is your sample representative of the overall population?** Are all the points independent of each other? Or do you have a "basketball team in your sample" situation (if you are trying to assess student height).
    -   *Assessed by visually looking for non-randomness (clusters/patterns) in your data and residual plot.* <br><br>
3.  [**Normality of residuals**]{.underline} **- Are your residuals normally distributed around the line of best fit? Or are they skewed in some way?**
    -   *Assessed by the qq-plot and normality tests*
    -   If broken apply a transformation to a/the PREDICTOR and see if your new model does better<br><br>
4.  [**Equal variance of residuals**]{.underline} **- no matter what your predictor/response - are your points around the same distance from the line? Or do you see "fanning out" / "bow-tie shapes"**
    -   *Assessed by visual inspection of residual plot and heteroskadisity F-test.*
    -   If broken apply a transformation to a/the PREDICTOR and see if your new model does better <br><br>

<br><br> 

### Coefficient of Determination $R^2$ {.unnumbered}

We could look first at the coefficient of variation for each model in the model summary. e.g. which model explains more variation in your response variable.

<br> 

#### Standard $R^2$ {.unnumbered}


The amount of variability explained by your model. e.g. 90% of the variability in building height can be explained by the number of floors it has.

#### Adjusted $R^2$ {.unnumbered}


This is more appropriate for multiple regression as it takes into account the number of predictors to prevent overfitting. Read this! <https://online.stat.psu.edu/stat501/lesson/10/10.3> and see the lecture on transformations/AIC.

#### Where to find them {.unnumbered}


The easiest way is in the olsrr summary. Look at the statistics at the top.

```{r}
data("iris")
model <- lm(Sepal.Length~Sepal.Width,data=iris)
olsrr::ols_regress(model)
```


<br><br>

### MSE, RMSE, (root) mean squared error {.unnumbered}

This is the raw variability around the regression line in units of Y. You can see this as the residual mean squares in ANOVA, or at the summary at the top.

#### RMSE

Root mean squared error. Same thing but the square root (e.g. closer to the standard deviation)

<br><br>

### Information criteria:  AIC {.unnumbered}

(read more here: <https://online.stat.psu.edu/stat501/lesson/10/10.5>).

To compare regression models, some statistical software may also give values of statistics referred to as **information criterion** statistics. For regression models, these statistics combine information about the SSE, the number of parameters in the model, and the sample size. A low value, compared to values for other possible models, is good. Some data analysts feel that these statistics give a more realistic comparison of models than the ùê∂ùëù statistic because ùê∂ùëùtends to make models seem more different than they actually are.

This is a non parametric test that takes into account the number of predictors and the amount of data, so is often more robust to bad linear fits than $R^2$ (which needs LINE to be true)

Three information criteria that we present are called **Akaike‚Äôs Information Criterion** (**AIC**), the **Bayesian Information Criterion** (**BIC**) (which is sometimes called **Schwartz‚Äôs Bayesian Criterion** (**SBC**)), and **Amemiya‚Äôs Prediction Criterion** (**APC**). The respective formulas are as follows:

![](images/Screenshot 2024-04-24 at 3.17.31‚ÄØPM.png){width="70%"}

In the formulas, *n* = sample size and *p* = number of regression coefficients in the model being evaluated (including the intercept). Notice that the only difference between AIC and BIC is the multiplier of *p*, the number of parameters.

Each of the information criteria is used in a similar way ‚Äî in comparing two models, the model with the lower value is preferred. The 'raw' values have little physical meaning. For now, know that the lower the AIC, the "better" the model.

Let's compare two models now, using our transformed data:

```{r}

model                 <- lm(Sepal.Length~Sepal.Width,data=iris)
model.transformation  <- lm(Sepal.Length~sqrt(Sepal.Width),data=iris)

model1summary <- summary(model)
model2summary <- summary(model.transformation)

# Adjusted R2
paste("Model 1:",round(model1summary$adj.r.squared,2) )
paste("Model 2:",round(model2summary$adj.r.squared,2) )

# AIC
AIC(model,model.transformation)

```

# Multiple regression & model fitting

## MLR

This is very similar to simple regression, but you can add in extra predictors:

For example, this will predict sepal length, using TWO predictors (width and petal length).  See the MLR lecture for interpretation.

```{r}
MLRmodel <- lm(Sepal.Length ~ Sepal.Width + Petal.Length ,data=iris)

ols_regress(MLRmodel)
```



# Finding the "optimal model"

## Best subsets

There are many models/combinations of predictors that we could use to predict our response variable. We want to find the best model possible, but we also don\'t want to overfit.

So far, we manually compared two models. In fact there is a way to compare all the combinations of predictors. This is using the `ols_step_best_subset()` command.

Describe what the \"best subset\" method is doing. Hint, we will go over this in lectures, but also <https://online.stat.psu.edu/stat501/lesson/10/10.3>

First, decide every predictor you think might be useful and create a model using this.  

FOR YOUR PROJECT, CHOOSE 8 PREDICTORS MAX (or R takes too long)


```{r,eval=FALSE}

FullModel <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + Species ,data=iris)


BestSubsets <- ols_step_best_subset(FullModel)
BestSubsets

```

So now we can find a model that seems to have the lowest AIC, the highest R2 etc.  BUT YOU STILL HAVE TO CHECK FOR LINE!


